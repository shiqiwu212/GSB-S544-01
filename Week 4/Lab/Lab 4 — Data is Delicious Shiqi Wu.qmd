---
title: "Lab 4 — Data is Delicious"
author: "Shiqi Wu"
format:
  html:
    self-contained: true
    toc: true
    number-sections: true
jupyter: python3
execute:
  warning: false
  echo: true
  cache: false
---

**Lab 4 HTML (self-contained):** [D](https://github.com/%3CYOUR_USER%3E/%3CREPO%3E/blob/%3CCOMMIT_OR_BRANCH%3E/labs/lab04/lab4_data_is_delicious.html)[ata is Delicious — Week 202](https://github.com/shiqiwu212/GSB-S544-01/tree/13258354f786acb491a1cfecc8668d7b9d4a05d5/Week%204/Lab)

# 0. Setup & Utilities

```{python}
# AI assistance (Setup): An AI assistant provided structural guidance (web scraping,
# API integration, automation, fuzzy matching, and analysis) and suggested robust helpers (HTTP retry,
# day normalization) Plus environment-based secret handling. 

import os, re, time
import requests
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import pandas as pd
import matplotlib.pyplot as plt

try:
    from rapidfuzz import fuzz  # best accuracy
except Exception:
    from difflib import SequenceMatcher
    class fuzz:  # lightweight fallback with a compatible API (partial_ratio)
        @staticmethod
        def partial_ratio(a, b):
            a = (a or "").lower()
            b = (b or "").lower()
            if not a or not b:
                return 0
            if len(a) < len(b):
                short, long = a, b
            else:
                short, long = b, a
            best = 0.0
            Ls, Ll = len(short), len(long)
            for i in range(0, Ll - Ls + 1):
                score = SequenceMatcher(None, short, long[i:i+Ls]).ratio()
                if score > best:
                    best = score
            return int(best * 100)

BASE = "https://tastesbetterfromscratch.com"
CATEGORY = "/category/meal-plan/"
USER_AGENT = "Mozilla/5.0 (compatible; Lab4-DataIsDelicious/1.0)"

RAPID_API_KEY = os.getenv("RAPIDAPI_KEY", "")
RAPID_HOST = "tasty.p.rapidapi.com"
TASTY_SEARCH = f"https://{RAPID_HOST}/recipes/list"

HEADERS_WEB = {"User-Agent": USER_AGENT}
HEADERS_API = {
    "X-RapidAPI-Key": RAPID_API_KEY,
    "X-RapidAPI-Host": RAPID_HOST,
    "User-Agent": USER_AGENT
}

DAY_NAMES = ["monday","tuesday","wednesday","thursday","friday","saturday","sunday"]
def norm_day(s: str) -> str:
    s = (s or "").strip().lower()
    for d in DAY_NAMES:
        if d in s:
            return d.title()
    return ""

def http_get(url, headers=None, params=None, retries=3, timeout=20):
    for i in range(retries):
        r = requests.get(url, headers=headers, params=params, timeout=timeout)
        if r.status_code == 200:
            return r
        time.sleep(1.2 * (i+1))
    r.raise_for_status()
```

# 1. Data from unstructured websites

```{python}
# AI assistance (Web-scrape): An AI assistant proposed a resilient strategy to discover "Meal Plan Week N"
# across paginated category pages and a sibling-based parsing approach for day → recipe mapping. Selectors
# and validations were adapted and confirmed by the student.

# ---- helpers that DO NOT raise on 404 ----
def _get_allow_404(url, headers=None, timeout=20):
    try:
        r = requests.get(url, headers=headers or HEADERS_WEB, timeout=timeout)
        if r.status_code == 404:
            return None
        r.raise_for_status()
        return r
    except requests.RequestException:
        return None  # treat as not available

def find_week_link_safe(week: int, max_probe: int = 50) -> str:
    """
    Scan the meal-plan category pages, stopping gracefully at the first 404.
    If not found, try a few common slug patterns as fallbacks.
    """
    def scan_for_week(html: str) -> str:
        soup = BeautifulSoup(html, "lxml")
        for a in soup.select("a[href]"):
            txt = re.sub(r"\s+", " ", (a.get_text() or "")).strip().lower()
            if (str(week) in txt) and ("week" in txt) and ("meal" in txt):
                return urljoin(BASE, a["href"])
        return ""

    # page 1
    cat_index = urljoin(BASE, CATEGORY)
    r = _get_allow_404(cat_index, headers=HEADERS_WEB)
    if r is None:
        raise RuntimeError(f"Category index not reachable: {cat_index}")
    link = scan_for_week(r.text)
    if link:
        return link

    # subsequent pages until first 404 (stop probing)
    for p in range(2, max_probe + 1):
        page_url = urljoin(cat_index, f"./page/{p}/")
        r = _get_allow_404(page_url, headers=HEADERS_WEB)
        if r is None:   # 404 or network problem → stop probing further
            break
        link = scan_for_week(r.text)
        if link:
            return link

    # fallbacks: common slug patterns
    candidates = [
        f"{BASE}/meal-plan-week-{week}/",
        f"{BASE}/weekly-meal-plan-{week}/",
        f"{BASE}/meal-plan-{week}/",
    ]
    for url in candidates:
        r = _get_allow_404(url, headers=HEADERS_WEB)
        if r is not None:  # a valid page (200)
            return url

    raise RuntimeError(f"Meal Plan Week {week} not found under {CATEGORY} (scanned available pages; tried fallbacks).")

def parse_week_page(week_url: str) -> pd.DataFrame:
    html = http_get(week_url, headers=HEADERS_WEB).text
    soup = BeautifulSoup(html, "lxml")
    rows = []

    candidates = []
    for tag_name in ["h2","h3","h4","strong","p","li"]:
        candidates.extend(soup.find_all(tag_name))

    for tag in candidates:
        day = norm_day(tag.get_text())
        if not day:
            continue

        recipe_a = None
        cursor = tag
        for _ in range(12):
            cursor = cursor.find_next_sibling()
            if cursor is None:
                break
            link = cursor.find("a", href=True)
            if link:
                recipe_a = link
                break
        if not recipe_a:
            link = tag.find("a", href=True)
            if link:
                recipe_a = link
        if not recipe_a:
            continue

        name = re.sub(r"\s+", " ", recipe_a.get_text(strip=True))
        href = urljoin(BASE, recipe_a["href"])

        # Try to detect a nearby price string such as "$12", "Under $15", etc.
        search_zone = " ".join([
            tag.get_text(" ", strip=True),
            recipe_a.get_text(" ", strip=True),
            (recipe_a.parent.get_text(" ", strip=True) if recipe_a.parent else "")
        ])
        price = None
        m = re.search(r"\$\s*\d+(?:\.\d{1,2})?", search_zone, flags=re.I)
        if not m and recipe_a.parent:
            nxt = recipe_a.parent.find_next_sibling()
            if nxt:
                m = re.search(r"\$\s*\d+(?:\.\d{1,2})?", nxt.get_text(" ", strip=True), flags=re.I)
        if m:
            price = m.group(0).replace(" ", "")

        rows.append({
            "Day of the Week": day,
            "Name of Recipe": name,
            "Link to Recipe": href,
            "Price of Recipe": price
        })

    df = pd.DataFrame(rows)
    if not df.empty:
        df = df.sort_values("Day of the Week").drop_duplicates("Day of the Week", keep="first")
        all_days = pd.DataFrame({"Day of the Week": [d.title() for d in DAY_NAMES]})
        df = all_days.merge(df, on="Day of the Week", how="left")
    return df

# ---- DEMO: choose week and build weekly_df safely (no 404 exceptions) ----
week_chosen = 202
week_url = find_week_link_safe(week_chosen)
weekly_df = parse_week_page(week_url)
print("Week URL:", week_url)
weekly_df
```

# 2. Data from an API

```{python}
# AI assistance (API): An AI assistant recommended environment-based secret management, capped page-size
# (≤100), and defensive guards so variables remain defined even if preconditions fail. 

import pandas as pd

# --- Always define the variable to avoid NameError later ---
monday_matches = pd.DataFrame()

# --- Preconditions: Step 1 must have produced weekly_df with a Monday entry ---
if "weekly_df" not in globals():
    raise RuntimeError("weekly_df is not defined. Run Step 1 before Step 2.")

if weekly_df.empty:
    raise RuntimeError("weekly_df is empty. Verify Step 1 scraped the week successfully.")

mon_list = weekly_df.loc[weekly_df["Day of the Week"]=="Monday","Name of Recipe"].dropna().head(1).tolist()
if not mon_list:
    print("Warning: Monday recipe not found on the selected week page; returning empty monday_matches.")
else:
    monday_query = mon_list[0]

    # --- API key check ---
    if not RAPID_API_KEY:
        print("Warning: RAPIDAPI_KEY is not set (Runtime → Environment Variables…). Returning empty monday_matches.")
    else:
        # --- Helpers (clean_query / tasty_search / match_recipe) ---
        def clean_query(name: str) -> str:
            q = name or ""
            q = re.sub(r"\(.*?\)|\[.*?\]|[^A-Za-z0-9\s]", " ", q)
            q = re.sub(r"\s+", " ", q).strip()
            parts = q.split()
            return " ".join(parts[:6]) if len(parts) > 6 else q

        def tasty_search(query: str, max_results: int = 100) -> pd.DataFrame:
            size = max(1, min(int(max_results), 100))
            params = {"q": query, "from": 0, "size": size}
            r = http_get(TASTY_SEARCH, headers=HEADERS_API, params=params)
            payload = r.json() if r.headers.get("content-type","").startswith("application/json") else {}
            items = payload.get("results", []) if isinstance(payload, dict) else []
            out = []
            for it in items:
                nutrition = it.get("nutrition") or {}
                out.append({
                    "tasty_id": it.get("id"),
                    "tasty_name": it.get("name"),
                    "tasty_description": it.get("description"),
                    "tasty_yields": it.get("yields"),
                    "tasty_servings_noun": it.get("servings_noun"),
                    "calories": nutrition.get("calories"),
                    "carbohydrates": nutrition.get("carbohydrates"),
                    "fat": nutrition.get("fat"),
                    "fiber": nutrition.get("fiber"),
                    "protein": nutrition.get("protein"),
                    "sugar": nutrition.get("sugar"),
                })
            return pd.DataFrame(out)

        def match_recipe(name: str, max_results: int = 100) -> pd.DataFrame:
            q = clean_query(name)
            if not q:
                return pd.DataFrame()
            df_local = tasty_search(q, max_results=max_results)
            df_local.insert(0, "query", q)
            return df_local

        # --- Do the Monday search; ensures monday_matches is set even if API returns nothing ---
        try:
            monday_matches = match_recipe(monday_query, max_results=100)
        except Exception as e:
            print(f"Warning: Tasty API call failed ({e}); returning empty monday_matches.")
            monday_matches = pd.DataFrame()

monday_matches
```

# 3. Automate it

```{python}
# AI assistance (Automation): An AI assistant proposed a modular pipeline—(1) discover the Week link,
# (2) parse the HTML plan, (3) call the API, (4) build a tidy table—so functions remain composable and
# free of week-specific hard-coding. The assistant also suggested resilient pagination: probe category
# pages in order and *gracefully stop* on 404 instead of raising. Clear error messages and small, testable

import re
import pandas as pd
from urllib.parse import urljoin

# ---- helpers that DO NOT raise on 404 ----
def _get_allow_404(url, headers=None, timeout=20):
    try:
        r = requests.get(url, headers=headers or HEADERS_WEB, timeout=timeout)
        if r.status_code == 404:
            return None
        r.raise_for_status()
        return r
    except requests.RequestException:
        return None  # treat as non-available

def find_week_link_safe(week: int, max_probe: int = 50) -> str:
    """
    Scan the category pages, stopping gracefully at the first 404.
    Also try a few common slug patterns as fallback.
    """
    def scan_for_week(html: str) -> str:
        soup = BeautifulSoup(html, "lxml")
        for a in soup.select("a[href]"):
            txt = re.sub(r"\s+", " ", (a.get_text() or "")).strip().lower()
            if (str(week) in txt) and ("week" in txt) and ("meal" in txt):
                return urljoin(BASE, a["href"])
        return ""

    # page 1
    cat_index = urljoin(BASE, CATEGORY)
    r = _get_allow_404(cat_index, headers=HEADERS_WEB)
    if r is None:
        raise RuntimeError(f"Category index not reachable: {cat_index}")
    link = scan_for_week(r.text)
    if link:
        return link

    # subsequent pages until 404
    for p in range(2, max_probe + 1):
        page_url = urljoin(cat_index, f"./page/{p}/")
        r = _get_allow_404(page_url, headers=HEADERS_WEB)
        if r is None:     # 404 or transient network → stop probing further
            break
        link = scan_for_week(r.text)
        if link:
            return link

    # fallback: try common slug patterns directly
    candidates = [
        f"{BASE}/meal-plan-week-{week}/",
        f"{BASE}/weekly-meal-plan-{week}/",
        f"{BASE}/meal-plan-{week}/",
    ]
    for url in candidates:
        r = _get_allow_404(url, headers=HEADERS_WEB)
        if r is not None:   # found a 200 OK
            return url

    raise RuntimeError(f"Meal Plan Week {week} not found under {CATEGORY} (scanned available pages; tried fallbacks).")

def get_weekly_plan(week: int) -> pd.DataFrame:
    return parse_week_page(find_week_link_safe(week))

# ---- ensure API helpers exist (if Step 2 not executed in this session) ----
if "clean_query" not in globals():
    def clean_query(name: str) -> str:
        q = name or ""
        q = re.sub(r"\(.*?\)|\[.*?\]|[^A-Za-z0-9\s]", " ", q)
        q = re.sub(r"\s+", " ", q).strip()
        parts = q.split()
        return " ".join(parts[:6]) if len(parts) > 6 else q

if "tasty_search" not in globals():
    def tasty_search(query: str, max_results: int = 100) -> pd.DataFrame:
        if not RAPID_API_KEY:
            raise RuntimeError("RAPIDAPI_KEY is not set (Runtime → Environment Variables…).")
        size = max(1, min(int(max_results), 100))
        params = {"q": query, "from": 0, "size": size}
        r = http_get(TASTY_SEARCH, headers=HEADERS_API, params=params)
        payload = r.json() if r.headers.get("content-type","").startswith("application/json") else {}
        items = payload.get("results", []) if isinstance(payload, dict) else []
        out = []
        for it in items:
            nutrition = it.get("nutrition") or {}
            out.append({
                "tasty_id": it.get("id"),
                "tasty_name": it.get("name"),
                "tasty_description": it.get("description"),
                "tasty_yields": it.get("yields"),
                "tasty_servings_noun": it.get("servings_noun"),
                "calories": nutrition.get("calories"),
                "carbohydrates": nutrition.get("carbohydrates"),
                "fat": nutrition.get("fat"),
                "fiber": nutrition.get("fiber"),
                "protein": nutrition.get("protein"),
                "sugar": nutrition.get("sugar"),
            })
        return pd.DataFrame(out)

if "match_recipe" not in globals():
    def match_recipe(name: str, max_results: int = 100) -> pd.DataFrame:
        q = clean_query(name)
        if not q:
            return pd.DataFrame()
        df_local = tasty_search(q, max_results=max_results)
        df_local.insert(0, "query", q)
        return df_local

def get_mealplan_data(week: int, max_results_each: int = 100) -> pd.DataFrame:
    base = get_weekly_plan(week)
    all_rows = []
    for _, row in base.iterrows():
        day = row["Day of the Week"]
        name = row["Name of Recipe"]
        link = row["Link to Recipe"]
        price = row["Price of Recipe"]
        matches = match_recipe(name, max_results=max_results_each)
        if matches.empty:
            all_rows.append({
                "week": week, "day": day, "recipe_name": name, "recipe_url": link, "recipe_price": price,
                "tasty_id": None, "tasty_name": None,
                "calories": None, "carbohydrates": None, "fat": None, "fiber": None, "protein": None, "sugar": None
            })
        else:
            for _, m in matches.iterrows():
                all_rows.append({
                    "week": week, "day": day, "recipe_name": name, "recipe_url": link, "recipe_price": price,
                    "tasty_id": m["tasty_id"], "tasty_name": m["tasty_name"],
                    "calories": m["calories"], "carbohydrates": m["carbohydrates"],
                    "fat": m["fat"], "fiber": m["fiber"], "protein": m["protein"], "sugar": m["sugar"]
                })
    return pd.DataFrame(all_rows)

# ---- run the required call for Week 202 ----
df = get_mealplan_data(202)
len(df), df.head(10)
```

# 4. Add a column with fuzzy matching

```{python}
# AI assistance (Fuzzy matching): An AI assistant suggested a maintainable meat-word list and a fuzzy strategy
# to reduce false negatives for typos. 

MEAT_WORDS = [
    "chicken","beef","steak","pork","bacon","turkey","sausage","ham","lamb",
    "salmon","tuna","shrimp","fish","meatball","chorizo","prosciutto","pepperoni",
]

def is_vegetarian(name: str, threshold: int = 88) -> bool:
    s = (name or "").lower()
    for w in MEAT_WORDS:
        if w in s:
            return False
        if fuzz.partial_ratio(s, w) >= threshold:
            return False
    return True

df["is_vegetarian"] = df["recipe_name"].apply(is_vegetarian)
df.head(10)
```

# 6. Analyze

```{python}
# AI assistance (Analysis): Suggested median-by-day visualization with weekday categorical order and vegetarian markers.
# Also proposed a parameterized METRIC selector with numeric labels; figure renders self-contained in HTML.

METRIC = "calories"  # change to "protein" or "sugar" if desired
df_source = df_clean if "df_clean" in globals() else df
if METRIC not in df_source.columns:
    raise ValueError(f"Selected METRIC '{METRIC}' not found in columns.")

plot_data = (
    df_source[df_source["week"] == 202]
      .dropna(subset=[METRIC])
      .groupby("day", as_index=False)
      .agg(median_value=(METRIC, "median"),
           any_veg=("is_vegetarian", "max"))
)

day_order = ["Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"]
plot_data["day"] = pd.Categorical(plot_data["day"], categories=day_order, ordered=True)
plot_data = plot_data.sort_values("day")

plt.figure(figsize=(9, 4.8))
plt.bar(plot_data["day"].astype(str), plot_data["median_value"])
for i, r in plot_data.reset_index(drop=True).iterrows():
    v = r["median_value"]
    if pd.notna(v):
        plt.text(i, v * 1.01, f"{v:.0f}", ha="center", va="bottom", fontsize=9)
    if r["any_veg"]:
        plt.text(i, v * 1.10 if pd.notna(v) else 0, "VEG", ha="center", va="bottom", fontsize=9)

title_metric = METRIC.capitalize()
plt.title(f"Mealplan 202 — Median {title_metric} by Day (Tasty matches)")
plt.xlabel("Day of Week")
plt.ylabel(f"Median {title_metric}")
plt.tight_layout()
plt.show()
```