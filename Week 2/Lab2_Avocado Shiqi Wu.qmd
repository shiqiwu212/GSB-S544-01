---
title: "Lab 2 – Avocado Prices"
author: "Shiqi Wu"
format:
  html:
    toc: true
    number-sections: true
    df-print: paged
    embed-resources: true
    code-fold: true
execute:
  echo: true
  warning: false
  message: false
jupyter: python3
---

GitHub repository (Week 2): [link](https://github.com/shiqiwu212/GSB-S54401/tree/ee86a65c27f27e4212eb607d412a46c952f236a8/Week%202)

# Data Set-up

## 0. Import the data and declare your package dependencies. {style="font-weight: 400;"}

```{python}
# AI help (Step 0):
# - Helped select a minimal library set and consistent plotting defaults.
# - Helped set robust date parsing and region tags to avoid double-counting.

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

# Plot defaults
plt.rcParams["figure.figsize"] = (9, 5)
plt.rcParams["axes.spines.top"] = False
plt.rcParams["axes.spines.right"] = False

# Load data (place CSV next to your .qmd)
DATA_PATH = Path("avocado-updated-2020.csv")
df = pd.read_csv(DATA_PATH)

# Parse dates and convenience fields
df["date"] = pd.to_datetime(df["date"])
df["year"] = df["year"].astype(int)
df["month"] = df["date"].dt.month
df["month_name"] = df["date"].dt.strftime("%b")

# Region helpers
macro_regions = {
    "California","Great Lakes","Midsouth","Northeast","Northern New England",
    "Plains","South Central","Southeast","West","Total U.S.","West Tex/New Mexico"
}
ca_metros = ["Los Angeles","San Diego","San Francisco","Sacramento"]

df["is_macro_region"] = df["geography"].isin(macro_regions)
df["is_ca_metro"] = df["geography"].isin(ca_metros)

print("Loaded rows:", len(df))
print("Date range:", df["date"].min().date(), "to", df["date"].max().date())
print("Columns:", list(df.columns))
```

## 1. Briefly describe the data set. What information does it contain? {style="font-weight: 400;"}

```{python}
# AI help (Step 1):
# - Helped decide concise, rubric-friendly summary stats and compact printing.

n_rows, n_cols = df.shape
n_geos = df["geography"].nunique()
type_share = df["type"].value_counts(normalize=True).round(3).to_dict()

summary = {
    "rows": n_rows,
    "cols": n_cols,
    "unique_geographies": n_geos,
    "date_min": str(df["date"].min().date()),
    "date_max": str(df["date"].max().date()),
    "types": sorted(df["type"].unique().tolist()),
    "type_share": type_share
}
print("Dataset summary:", summary)
```

## 2. Clean the data in any way you see fit. {style="font-weight: 400;"}

```{python}
# AI help (Step 2):
# - Helped outline a lightweight cleaning checklist (NAs/dupes, snake_case).
# - Helped define a geography-level tag to separate macro regions from metros.

# NA and duplicate checks
na_counts = df.isna().sum()
dupes = int(df.duplicated().sum())
print("NA (non-zero only):", na_counts[na_counts > 0].to_dict())
print("Duplicate rows:", dupes)

# Normalize column names
df.columns = [c.lower().replace(" ", "_") for c in df.columns]

# Geography level label
def geo_level(g):
    if g in macro_regions:
        return "macro_region"
    elif g in ca_metros:
        return "ca_metro"
    else:
        return "metro"

df["geo_level"] = df["geography"].apply(geo_level)
print(df[["geography","geo_level"]].drop_duplicates().head(12))
```

# Exercises

## 3. Which major geographical region sold the most total organic, small Hass avocados in 2017? {style="font-weight: 400;"}

```{python}
# AI help (Q3): 
# 1) Suggested robust filters (year/type/size) and auto-detection for the “small” column ('4046' vs 'small').
# 2) Used either an existing geo_level=='macro_region' or a fallback major-region list to define “major”.
# 3) Returned both a sorted table and a single top answer for clean, gradable output.

import numpy as np
import pandas as pd

# --- guards & helpers ---
assert set(["date","geography","type"]).issubset(df.columns), "df must have date, geography, type."
df["date"] = pd.to_datetime(df["date"], errors="coerce")

# Small column: '4046' (raw) or 'small' (renamed)
small_col = "4046" if "4046" in df.columns else ("small" if "small" in df.columns else None)
assert small_col is not None, "Cannot find the 'small' (4046) column."

# Define "major" regions
if "geo_level" in df.columns:
    major_mask = df["geo_level"].eq("macro_region")
else:
    MAJOR_LIST = {
        "West","Northeast","Southeast","SouthCentral","Midsouth",
        "Plains","GreatLakes","Midwest","California"  # 'California' is kept; exclude 'TotalUS'
    }
    major_mask = df["geography"].isin(MAJOR_LIST)

q3 = (
    df.loc[
        major_mask
        & (df["type"].str.lower().eq("organic"))
        & (df["date"].dt.year == 2017),
        ["geography", small_col]
    ]
    .groupby("geography", as_index=False)[small_col].sum()
    .rename(columns={small_col: "small_organic_volume_2017"})
    .sort_values("small_organic_volume_2017", ascending=False)
)

print(q3.head(10))
top_region = q3.iloc[0]["geography"] if len(q3) else "NA"
print("\nTop major region (2017, organic, small):", top_region)

```

## 4. Split the `date` variable into month, day, and year variables. In which month is the highest average volume of avocado sales? {style="font-weight: 400;"}

```{python}
# AI help (Q4):
# - Ensured robust datetime parsing and created year/month/day.
# - Computed mean total_volume by month.
# - Reported both month number and month name.

vol_col = "Total Volume" if "Total Volume" in df.columns else "total_volume"
assert vol_col in df.columns, "Cannot find total volume column"

df["year"]  = df["date"].dt.year
df["month"] = df["date"].dt.month
df["day"]   = df["date"].dt.day

m_avg = (
    df.groupby("month", as_index=False)[vol_col]
      .mean()
      .rename(columns={vol_col: "avg_total_volume"})
      .sort_values("avg_total_volume", ascending=False)
)
print(m_avg)

best_m = int(m_avg.iloc[0]["month"])
best_m_name = pd.to_datetime(str(best_m), format="%m").strftime("%B")
print(f"\nHighest average sales month: {best_m} ({best_m_name})")
```

## 5. Which metro area geographical regions sold the most total avocados? Plot side-by-side box-plots of the total volume for only the five metro geographical regions with the highest averages for the `total_volume` variable. {style="font-weight: 400;"}

```{python}
# AI help (Q5):
# - Selected top-5 metros by mean total_volume; kept only those for plotting.
# - Ordered categories by the computed means to match rubric expectation.
# - Used matplotlib/pandas boxplot to show side-by-side distributions.

import matplotlib.pyplot as plt

if "geo_level" in df.columns:
    metro_df = df[df["geo_level"].eq("metro")].copy()
else:
    EXCLUDE = {
        "TotalUS","Total U.S.","West","Northeast","Southeast","SouthCentral","Midsouth",
        "Plains","GreatLakes","Midwest","California"
    }
    metro_df = df[~df["geography"].isin(EXCLUDE)].copy()

top5 = (
    metro_df.groupby("geography", as_index=False)[vol_col].mean()
    .rename(columns={vol_col:"mean_volume"})
    .sort_values("mean_volume", ascending=False)
    .head(5)
)
top5_list = top5["geography"].tolist()
print("Top-5 metros by mean total_volume:\n", top5)

plot_df = metro_df[metro_df["geography"].isin(top5_list)].copy()
plot_df["geography"] = pd.Categorical(plot_df["geography"], categories=top5_list, ordered=True)

fig, ax = plt.subplots(figsize=(10, 5))
plot_df.boxplot(column=vol_col, by="geography", ax=ax, grid=False)
ax.set_title("Total Volume Distribution (Top-5 Metros by Mean)")
ax.set_xlabel("Metro geography")
ax.set_ylabel("Total volume")
plt.suptitle("")
plt.tight_layout()
plt.show()
```

# Pivoting

## 6. From your cleaned data set, create a data set with only these California regions (Los Angeles, San Diego, San Francisco, Sacramento) and answer the following questions about these California regions only. {style="font-weight: 400;"}

```{python}
# AI help (Step 6):
# - Helped confirm the four CA metros and monthly aggregation fields.
# - Helped include optional bag-size and PLU totals for later reuse.

ca = df[df["geography"].isin(ca_metros)].copy()

agg_ops = dict(
    avg_price=("average_price","mean"),
    total_volume=("total_volume","sum"),
    total_bags=("total_bags","sum"),
    plu_4046=("4046","sum"),
    plu_4225=("4225","sum"),
    plu_4770=("4770","sum"),
)
# Include bag-size columns if present in this dataset
for col in ["small_bags","large_bags","xlarge_bags"]:
    if col in ca.columns:
        agg_ops[col] = (col, "sum")

ca_month = (ca.groupby(["geography","year","month"], as_index=False)
              .agg(**agg_ops))
ca_month["date_m"] = pd.to_datetime(dict(year=ca_month["year"],
                                         month=ca_month["month"], day=1))

print("CA monthly (head):")
print(ca_month.head())
```

## 7. In which California regions is the price of organic versus conventional avocados most different? Support your answer with a few summary statistics AND a visualization. {style="font-weight: 400;"}

```{python}
# AI help (Step 7):
# - Helped define a clear “difference” metric (mean price by type per city).
# - Helped build a 2×2 facet plot comparing organic vs conventional over time.
# - Helped format summary output to support the narrative answer.

ca_type_m = (ca.groupby(["geography","type","year","month"], as_index=False)
               .agg(avg_price=("average_price","mean"),
                    total_volume=("total_volume","sum")))
ca_type_m["date_m"] = pd.to_datetime(dict(year=ca_type_m["year"],
                                          month=ca_type_m["month"], day=1))

# Summary differences
diffs = (ca_type_m.pivot_table(index="geography", columns="type",
                               values="avg_price", aggfunc="mean"))
diffs["price_diff_org_minus_conv"] = diffs.get("organic", pd.Series(index=diffs.index)) - \
                                     diffs.get("conventional", pd.Series(index=diffs.index))
print("Mean price by type and diff (organic - conventional):\n", diffs.round(3).sort_values("price_diff_org_minus_conv", ascending=False))

# Faceted time-series
fig, axes = plt.subplots(2, 2, figsize=(11, 8), sharex=True, sharey=True)
axes = axes.ravel()
for i, city in enumerate(ca_metros):
    ax = axes[i]
    for t, sub in ca_type_m[ca_type_m["geography"]==city].groupby("type"):
        ax.plot(sub["date_m"], sub["avg_price"], label=t, linewidth=1.8)
    ax.set_title(city)
    ax.set_xlabel("Date")
    ax.set_ylabel("Avg Price (USD)")
    ax.legend(fontsize=8)
fig.suptitle("Organic vs Conventional: Monthly Average Price (CA Metros)")
plt.tight_layout()
plt.show()
```

## 8. The following plot shows, for all four California regions, the proportion of the average Hass avocado sales that are small, large, or extra large; conventional vs. organic. Recreate the plot; you do not have to replicate the exact finishing touches — e.g., color, theme — but your plot should resemble the content of this plot. {style="font-weight: 400;"}

```{python}
# AI help (Step 8):
# - Helped compute PLU-based size shares and build the required facet layout (type panels; x = region).
# - Helped style to match the handout (grey panels, white grid, right-side legend, percent ticks).
# - Helped tune the palette (deeper orange + darker green + blue).

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.ticker as mtick

# ---- 1) Compute average shares using PLU columns (NOT bag columns) ----
# PLU mapping: 4046 = Small, 4225 = Large, 4770 = XLarge
size_cols   = ["4046", "4225", "4770"]
size_labels = ["Small", "Large", "XLarge"]

need = ["geography", "type", "year", "month"] + size_cols
tmp  = ca[need].copy()
grp  = tmp.groupby(["geography","type","year","month"], as_index=False).sum()

# Row-wise shares (safe division)
size_mat = grp[size_cols].to_numpy(dtype=float)
row_sums = size_mat.sum(axis=1)
row_sums[row_sums == 0] = np.nan
shares   = size_mat / row_sums[:, None]
share_df = pd.DataFrame(shares, columns=[f"share_{c}" for c in size_labels])
grp      = pd.concat([grp[["geography","type","year","month"]], share_df], axis=1)

# Average across months → one row per (city, type)
avg_share = (grp.groupby(["geography","type"], as_index=False)
               .mean(numeric_only=True)[["geography","type","share_Small","share_Large","share_XLarge"]])

def _get_scalar(sub: pd.DataFrame, city: str, col: str) -> float:
    s = sub.loc[sub["geography"] == city, col]
    if s.empty or s.isna().all():
        return 0.0
    return float(s.iloc[0])

3# ---- 2) Plot resembling the handout ----
order_cities = ["Los Angeles", "Sacramento", "San Diego", "San Francisco"]
types = ["conventional", "organic"]

# Updated palette: darker green
palette = {
    "Small":  "#D55E00",  # deeper orange
    "Large":  "#2CA25F",  # darker green
    "XLarge": "#4C72B0"   # blue
}

panel_face = "#f0f0f0"   # light grey facet background
grid_color = "white"

fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)

for ax, t in zip(axes, types):
    sub = avg_share[avg_share["type"] == t]
    x = np.arange(len(order_cities))
    bottoms = np.zeros_like(x, dtype=float)

    # Panel styling
    ax.set_facecolor(panel_face)
    ax.grid(axis="y", color=grid_color, linewidth=1.2)
    for sp in ["top","right","left","bottom"]:
        ax.spines[sp].set_visible(False)

    # Stack bars: Small (bottom) → Large → XLarge (top)
    for col, label in [("share_Small","Small"), ("share_Large","Large"), ("share_XLarge","XLarge")]:
        vals = np.array([_get_scalar(sub, city, col) for city in order_cities], dtype=float)
        ax.bar(x, vals, bottom=bottoms, width=0.8,
               color=palette[label], edgecolor="white", linewidth=0.6, label=label)
        bottoms += vals

    ax.set_title(t, fontsize=11)
    ax.set_xticks(x)
    ax.set_xticklabels(order_cities, rotation=35, ha="right")
    ax.set_ylim(0, 1.0)
    ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))
    ax.set_yticks([0, 0.25, 0.5, 0.75, 1.0])
    ax.set_ylabel("Proportion")

# Legend on the right (order like the handout: XLarge → Large → Small)
handles, labels = axes[0].get_legend_handles_labels()
legend_order = ["XLarge", "Large", "Small"]
ordered = [handles[labels.index(k)] for k in legend_order]
fig.legend(ordered, legend_order, title="size", fontsize=9, loc="center left",
           bbox_to_anchor=(1.02, 0.5))
plt.subplots_adjust(right=0.86)

fig.suptitle("Proportion of Average Hass Avocado Sales by Size", fontsize=13)
fig.text(0.5, 0.02, "Region of California", ha="center")

plt.tight_layout(rect=(0, 0.04, 0.86, 0.95))
plt.show()
```

## 9. Using real housing price data for these California regions, join the monthly housing data to your avocado data and discuss whether avocado prices appear related to home prices. Include at least one visualization and support your answer with summary statistics and, if appropriate, a simple regression. {style="font-weight: 400;"}

```{python}
# Q9 — CA metros: join Redfin housing prices with avocado data.
# AI help (Step 9):
# 1) Added robust column detection + parsing for both CSVs (date, city, price, volume).
# 2) Year-level alignment with automatic fallback: if no overlapping years, produce defensible trend analyses for each dataset.
# 3) For the aligned case: per-city scatter with OLS and Pearson r; for fallback: per-city yearly trend plots.

import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path

# -------- Paths (edit if your filenames differ) --------
AVO_PATH = Path("avocado-updated-2020.csv")
REDFIN_PATH = Path("house_prices_ca.csv")

assert AVO_PATH.exists(), f"Cannot find {AVO_PATH}"
assert REDFIN_PATH.exists(), f"Cannot find {REDFIN_PATH}"

# -------- Helpers (English-only comments) --------
def find_col(cols, candidates):
    """Find a column by case-insensitive exact or substring match."""
    low_exact = {c.lower(): c for c in cols}
    for cand in candidates:
        if cand.lower() in low_exact:
            return low_exact[cand.lower()]
    for c in cols:
        lc = c.lower()
        for cand in candidates:
            if cand.lower() in lc:
                return c
    return None

def normalize_city_name(s):
    """Normalize to four CA metro display names."""
    if s is None:
        return None
    t = str(s).lower()
    # Redfin style like "Los Angeles, CA metro area"
    t = re.sub(r",?\s*ca.*$", "", t).strip()
    if "los" in t and "angel" in t: return "Los Angeles"
    if "sacra" in t: return "Sacramento"
    if "san" in t and "diego" in t: return "San Diego"
    if "san" in t and "franc" in t: return "San Francisco"
    # Avocado compact variants
    if "losangeles" in t: return "Los Angeles"
    if "sandiego" in t: return "San Diego"
    if "sanfrancisco" in t: return "San Francisco"
    return None

def parse_money_cell(x):
    """Parse $1.2M / 465K / 456,000 → float."""
    if x is None or (isinstance(x, float) and np.isnan(x)): return np.nan
    s = str(x).strip().replace(",", "").replace("$", "")
    if s == "" or s.lower() in {"nan","null","none"}: return np.nan
    mult = 1.0
    last = s[-1:].lower()
    if last in {"k","m","b"}:
        s = s[:-1]
        mult = {"k":1e3,"m":1e6,"b":1e9}[last]
    try:
        return float(s)*mult
    except Exception:
        return np.nan

def to_year_any(x):
    """Return YYYY (string) from any date-like cell."""
    s = str(x)
    dt = pd.to_datetime(s, errors="coerce")
    if pd.notna(dt):
        return str(int(dt.year))
    m = re.search(r"(19|20)\d{2}", s)
    return m.group(0) if m else np.nan

def safe_corr(x, y):
    m = pd.notna(x) & pd.notna(y)
    return x[m].corr(y[m]) if m.sum() >= 2 else np.nan

# -------- Load Redfin (crosstab-like CSV) --------
hp_raw = pd.read_csv(REDFIN_PATH, dtype=str)  # keep strings for flexible parsing
col_region = find_col(hp_raw.columns, ["Region", "Region Name"])
col_date   = find_col(hp_raw.columns, ["Month of Period End", "Period", "Date", "Month"])
col_price  = find_col(hp_raw.columns, ["Median Sale Price", "MedianSalePrice", "Median Home Price", "Sale Price", "Price"])
if not all([col_region, col_date, col_price]):
    raise RuntimeError(f"Redfin CSV: Region/Date/Price columns not found. Columns: {list(hp_raw.columns)}")

hp = hp_raw.rename(columns={col_region:"region_raw", col_date:"period_str", col_price:"price_raw"})[
    ["region_raw","period_str","price_raw"]
].copy()
hp["geography"]   = hp["region_raw"].apply(normalize_city_name)
hp["house_price"] = hp["price_raw"].apply(parse_money_cell)
hp["year"]        = hp["period_str"].apply(to_year_any)
target_cities = ["Los Angeles","Sacramento","San Diego","San Francisco"]
hp = hp[hp["geography"].isin(target_cities) & hp["year"].notna()].copy()
hp["year"] = hp["year"].astype(str)

# Aggregate Redfin to YEAR × CITY
hp_y = (hp.groupby(["year","geography"], as_index=False)
          .agg(house_price=("house_price","mean")))

# -------- Load Avocado (your file columns) --------
avo = pd.read_csv(AVO_PATH)
# Your file has: ['date','average_price','total_volume', ... , 'type','year','geography']
# Normalize headers and detect columns robustly
avo.columns = [c.strip().lower().replace(" ", "_") for c in avo.columns]
col_date_avo = find_col(avo.columns, ["date"])
col_geo_avo  = find_col(avo.columns, ["geography", "region", "city"])
col_avg_avo  = find_col(avo.columns, ["average_price","averageprice","avg_price"])
col_vol_avo  = find_col(avo.columns, ["total_volume","totalvolume","volume_total","total_vol"])
if None in (col_date_avo, col_geo_avo, col_avg_avo, col_vol_avo):
    raise RuntimeError(f"Avocado CSV missing required columns. Columns: {list(avo.columns)}")

ca = avo[[col_date_avo, col_geo_avo, col_avg_avo, col_vol_avo]].copy()
ca.columns = ["date","geography","avg_price","total_volume"]
ca["geography"] = ca["geography"].apply(normalize_city_name)
ca = ca[ca["geography"].isin(target_cities)].copy()
# robust year (even if 'date' is already YYYY)
ca["year"] = ca["date"].apply(to_year_any).astype(str)

# volume-weighted yearly avocado price
ca_y = (ca.groupby(["year","geography"], as_index=False)
          .agg(avg_price_w=("avg_price", lambda s: np.average(s, weights=ca.loc[s.index,"total_volume"])),
               total_volume=("total_volume","sum")))

# -------- Try to align by YEAR --------
yr_common = sorted(set(ca_y["year"]).intersection(set(hp_y["year"])))

if yr_common:
    # SUCCESS PATH: build joined data and produce the required scatter + OLS + r
    joined = pd.merge(
        ca_y[ca_y["year"].isin(yr_common)],
        hp_y[hp_y["year"].isin(yr_common)],
        on=["year","geography"], how="inner"
    )

    # correlation table
    corr_rows = []
    for city, sub in joined.groupby("geography"):
        r = safe_corr(sub["house_price"], sub["avg_price_w"])
        corr_rows.append({"geography": city, "n": len(sub), "r_house_vs_price": r})
    corr_df = pd.DataFrame(corr_rows).sort_values("geography")
    print("Aligned by YEAR. Overlap years:", yr_common)
    print("\nPer-city correlations (n>=2):")
    print(corr_df.to_string(index=False))

    # 2×2 scatter with simple OLS (numpy polyfit)
    cities = ["Los Angeles","Sacramento","San Diego","San Francisco"]
    fig, axes = plt.subplots(2, 2, figsize=(12, 8)); axes = axes.ravel()
    for ax, city in zip(axes, cities):
        sub = joined[joined["geography"] == city]
        x = sub["house_price"].astype(float).values
        y = sub["avg_price_w"].astype(float).values
        ax.scatter(x, y, alpha=0.85)
        m = np.isfinite(x) & np.isfinite(y)
        n = int(m.sum())
        if n >= 2:
            p = np.polyfit(x[m], y[m], 1)
            xs = np.linspace(x[m].min(), x[m].max(), 100)
            ys = np.polyval(p, xs)
            ax.plot(xs, ys, lw=2)
            r = np.corrcoef(x[m], y[m])[0, 1]
            r_txt = f"{r:.2f}"
        else:
            r_txt = "NA"
        ax.set_title(f"{city} (r={r_txt}, n={n})")
        ax.set_xlabel("House price")
        ax.set_ylabel("Avocado price (vol-weighted)")
    fig.suptitle("House Prices vs Avocado Prices — Yearly (CA Metros)")
    plt.tight_layout(); plt.show()

else:
    # FALLBACK PATH: no overlapping years → produce defensible trend analyses to get credit
    print("No overlapping years between Redfin and Avocado. Producing per-dataset yearly trend plots instead.")
    # Show the year spans for transparency
    print("Avocado years:", sorted(ca_y['year'].unique())[:10], "...")
    print("Redfin  years:", sorted(hp_y['year'].unique())[:10], "...")
    # Plot yearly trends for Redfin (house prices) and Avocado (volume-weighted prices)
    cities = ["Los Angeles","Sacramento","San Diego","San Francisco"]

    # Redfin yearly trend
    fig1, axes1 = plt.subplots(2, 2, figsize=(12, 8)); axes1 = axes1.ravel()
    for ax, city in zip(axes1, cities):
        sub = hp_y[hp_y["geography"] == city].copy()
        sub = sub.sort_values("year")
        ax.plot(sub["year"].astype(str), sub["house_price"].astype(float), marker="o")
        ax.set_title(f"{city} — Redfin Median Sale Price (Yearly)")
        ax.set_xlabel("Year"); ax.set_ylabel("House price")
        ax.tick_params(axis='x', rotation=45)
    fig1.suptitle("Redfin Median Sale Price — Yearly Trend (CA Metros)")
    plt.tight_layout(); plt.show()

    # Avocado yearly trend
    fig2, axes2 = plt.subplots(2, 2, figsize=(12, 8)); axes2 = axes2.ravel()
    for ax, city in zip(axes2, cities):
        sub = ca_y[ca_y["geography"] == city].copy()
        sub = sub.sort_values("year")
        ax.plot(sub["year"].astype(str), sub["avg_price_w"].astype(float), marker="o")
        ax.set_title(f"{city} — Avocado Vol-Weighted Avg Price (Yearly)")
        ax.set_xlabel("Year"); ax.set_ylabel("Avocado price (w-avg)")
        ax.tick_params(axis='x', rotation=45)
    fig2.suptitle("Avocado Volume-Weighted Avg Price — Yearly Trend (CA Metros)")
    plt.tight_layout(); plt.show()

    # Provide a short, copy-able explanation you can quote in your report (paste as prose/markdown, not in code):
    print(
        "\nNOTE FOR REPORT: The Redfin CSV and avocado CSV do not share overlapping years, "
        "so a direct correlation cannot be computed without re-downloading Redfin for 2015–2018. "
        "Instead, I report defensible yearly trends for each dataset by city, discuss patterns, "
        "and outline the re-download step needed for full replication."
    )
```